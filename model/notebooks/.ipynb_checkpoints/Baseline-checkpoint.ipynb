{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read image into tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = os.path.abspath('../../data/png/')\n",
    "IMAGE_FILE_WILDCARD = '/*.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# filenames = glob(IMAGE_DIR + IMAGE_FILE_WILDCARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a queue of file names including all the images files in the relative\n",
    "# image directory.\n",
    "filenames = tf.train.match_filenames_once(IMAGE_DIR + IMAGE_FILE_WILDCARD)\n",
    "filename_queue = tf.train.string_input_producer(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_reader = tf.WholeFileReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a whole file from the queue\n",
    "filename, image_file = image_reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the image as a PNG file, this will turn it into a Tensor which we can\n",
    "# then use in training.\n",
    "image = tf.image.decode_png(image_file, channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756435.2.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.8.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756693.7.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.4.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.8.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756435.4.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756693.9.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.2.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.1.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.5.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756435.8.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.6.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.3.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.9.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.3.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756693.8.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.7.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.7.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756435.3.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756435.5.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756693.5.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.4.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.2.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.1.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.9.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756435.1.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756693.3.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756435.6.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.6.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.0.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_02.1506756693.5.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506750812.0.html.png-clipped.png'\n",
      "(1684, 1190, 3)\n",
      "b'/Users/ericng/Workspace/doc-pic-classifier/data/png/doc_template_01.1506756435.7.html.png-clipped.png'\n",
      "no more elements\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "# Start a new session to show example output.\n",
    "with tf.Session() as sess:\n",
    "    # Required to get the filename matching to run.\n",
    "    # lesson learned... https://stackoverflow.com/questions/44143139/tensorflow-operation-tf-train-match-filenames-once-not-working\n",
    "    init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "\n",
    "    # Coordinate the loading of image files.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Get an image tensor and print its value.\n",
    "            filename_tensor, image_tensor = sess.run([filename, image])\n",
    "            print(image_tensor.shape)\n",
    "            print(filename_tensor)\n",
    "            \n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        # This will be raised when you reach the end of an epoch (i.e. the\n",
    "        # iterator has no more elements).\n",
    "        print('no more elements')\n",
    "\n",
    "    # Perform any end-of-epoch computation here.\n",
    "    print('Done training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:doc-gen-classifier]",
   "language": "python",
   "name": "conda-env-doc-gen-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
