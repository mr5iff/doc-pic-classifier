{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_DIR = os.path.abspath('../../data/png/train')\n",
    "VALIDATE_IMAGE_DIR = os.path.abspath('../../data/png/validate')\n",
    "TEST_IMAGE_DIR = os.path.abspath('../../data/png/test')\n",
    "\n",
    "MODEL_PATH = os.path.abspath('../baseline.h5')\n",
    "\n",
    "IMAGE_FILE_WILDCARD = '*/*.png'\n",
    "DOC_TEMPLATES_DIR = os.path.abspath('../../doc_pic_generator/templates/')\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "ORIGINAL_IMAGE_DIMENSION = (1684, 1190, 3)\n",
    "MODEL_IMAGE_DIMENSION = (224, 224, 3) # as VGG input is (224, 224, 3)\n",
    "\n",
    "categorical_classes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this classification task, we used a pre-trained model (VGG16) stacked with a fully connected layer of 128 units. The most of the VGG16 layers are freezed, except the last convulutional block (3 layers of Conv2D + 1 layer of maxpooling) is set to be trainable (to prevent overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Cropping2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.0e-05\n",
    "dropout_rate = .5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# freeze the weight in the VGG model\n",
    "trainable_VGG = VGG16(include_top=False)\n",
    "\n",
    "for layer in trainable_VGG.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in trainable_VGG.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.add(Lambda(lambda image: tf.image.resize_images(image, MODEL_IMAGE_DIMENSION[:-1]), input_shape=(None, None, 3), output_shape=MODEL_IMAGE_DIMENSION))\n",
    "model.add(trainable_VGG)\n",
    "\n",
    "# model.add(Conv2D(32, (5, 5), strides=(3, 3), input_shape=ORIGINAL_IMAGE_DIMENSION, padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), strides=(2, 2), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, (2, 2), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(512, (1, 1), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(categorical_classes, activation='softmax'))\n",
    "\n",
    "optimizer = optimizers.Adam(lr=learning_rate, clipnorm=1.)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_18 (Lambda)           (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               3211392   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 17,926,596\n",
      "Trainable params: 10,291,332\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "validate_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 471 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_IMAGE_DIR,  # this is the target directory\n",
    "#         target_size=MODEL_IMAGE_DIMENSION[:-1],\n",
    "        target_size=ORIGINAL_IMAGE_DIMENSION[:-1],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 157 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "        VALIDATE_IMAGE_DIR,  # this is the target directory\n",
    "#         target_size=MODEL_IMAGE_DIMENSION[:-1],\n",
    "        target_size=ORIGINAL_IMAGE_DIMENSION[:-1],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 157 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        VALIDATE_IMAGE_DIR,  # this is the target directory\n",
    "#         target_size=MODEL_IMAGE_DIMENSION[:-1],\n",
    "        target_size=ORIGINAL_IMAGE_DIMENSION[:-1],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 645s - loss: 0.8983 - acc: 0.6744 - val_loss: 0.3405 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 664s - loss: 0.1916 - acc: 0.9892 - val_loss: 0.0191 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 622s - loss: 0.0175 - acc: 1.0000 - val_loss: 3.6755e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbc27908>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "#         steps_per_epoch=10,\n",
    "        epochs=3,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking accuracy with test data where the model did not see in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00036661404010374099, 1.0]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [loss, accuracy]\n",
    "model.evaluate_generator(test_generator, test_generator.samples // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 100% accuracy from the unseen data.. the model is doing good (because the documents are quite standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = {val: key for key, val in train_generator.class_indices.items()}\n",
    "json.dump(class_indices, open(CLASS_INDICES_PATH, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify_doc_pic() is for end users to use the model to classify. It is available in baseline.py as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "MODEL_PATH = os.path.abspath('../baseline.h5')\n",
    "CLASS_INDICES_PATH = os.path.abspath('../class_indices.json')\n",
    "\n",
    "\n",
    "def classify_doc_pic(image_path, model=None, class_indices=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      image_path (str): file path of the image\n",
    "      model (Keras Model)\n",
    "      class_indices (dict): e.g. {\"0\": \"doc_template_01\", \"1\": \"doc_template_02\", \"2\": \"doc_template_03\", \"3\": \"doc_template_04\"}\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        'class_index': class_index, # e.g. 0\n",
    "        'class_name': class_indices[class_index] # e.g. \"doc_template_01\"\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    img = load_img(image_path)\n",
    "    img_arr = np.expand_dims(img_to_array(img), 0)\n",
    "\n",
    "    # load model if it is not load\n",
    "    if model is None:\n",
    "        model = keras.models.load_model(MODEL_PATH)\n",
    "        print('model loaded from: {}'.format(MODEL_PATH))\n",
    "\n",
    "    if class_indices is None:\n",
    "        class_indices = json.load(open(CLASS_INDICES_PATH, 'r'))\n",
    "        print('class indices loaded from: {}'.format(CLASS_INDICES_PATH))\n",
    "\n",
    "    class_index = np.asscalar(model.predict_classes(img_arr))\n",
    "\n",
    "    return {\n",
    "        'class_index': class_index,\n",
    "        'class_name': class_indices[class_index]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_index': 3, 'class_name': 'doc_template_04'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_doc_pic(os.path.abspath('../../data/png/test/doc_template_04/doc_template_04.1507524410.1.html.png-clipped.png'), model, class_indices)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:doc-gen-classifier]",
   "language": "python",
   "name": "conda-env-doc-gen-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
